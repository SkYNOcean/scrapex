    >>> from scrapex import Scraper, common
    >>> s = Scraper(use_cache = True)
    >>> doc = s.load('https://github.com/search?q=scraping')
    >>> 
    >>> print(doc.extract("//title"))
    Search · scraping · GitHub
    >>> 
    >>> print(doc.extract("//h3[contains(text(),'results')]").strip())
    59,256 repository results
    >>> 
    >>> listings = doc.q("//ul[@class='repo-list']/li")
    >>> print('number of listings on first page:', len(listings) )
    number of listings on first page: 10
    >>> 
    >>> for listing in listings:
    ...     print('repo name: ',listing.extract(".//div[contains(@class,'text-normal')]/a"))
    ... 
    repo name:  scrapinghub/portia
    repo name:  scrapy/scrapy
    repo name:  REMitchell/python-scraping
    repo name:  MontFerret/ferret
    repo name:  lorien/grab
    repo name:  tidyverse/rvest
    repo name:  lorien/awesome-web-scraping
    repo name:  scrapy/scrapely
    repo name:  soimort/you-get
    repo name:  code4craft/webmagic
    >>> 